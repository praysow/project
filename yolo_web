import streamlit as st
import cv2
import queue
import threading
import pyttsx3
import openai
from ultralytics import YOLO
from PIL import Image, ImageDraw, ImageFont
import numpy as np
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

# OpenAI API 키 설정
openai.api_key = 'YOUR_OPENAI_API_KEY'

# YOLO 모델 로드
model_path = 'best.pt'
model = YOLO(model_path)
model.model.names = ['빨간불', '초록불', '빨간불', '초록불', '자전거', '킥보드', '라바콘', '횡단보도']

# 클래스별 바운딩 박스 색상 지정
colors = {
    '빨간불': (255, 0, 0),
    '초록불': (0, 255, 0),
    '자전거': (0, 0, 0),
    '킥보드': (128, 0, 128),
    '라바콘': (255, 165, 0),
    '횡단보도': (255, 255, 255)
}

# 한글 폰트 경로 설정
font_path = "C:/Windows/Fonts/malgun.ttf"
font = ImageFont.truetype(font_path, 20)

# pyttsx3 엔진 초기화
engine = pyttsx3.init()
engine.setProperty('rate', 200)

# 큐 초기화
q = queue.Queue()

def speak():
    while True:
        text = q.get()
        if text is None:
            break
        engine.say(text)
        engine.runAndWait()

# 음성 출력 스레드 시작
speak_thread = threading.Thread(target=speak)
speak_thread.start()

# 상태 추적 변수
current_light_state = None  # 'red', 'green' 또는 None

# 랭체인 LLM 설정
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    openai_api_key=openai.api_key
)

prompt_template = PromptTemplate(
    input_variables=["command"],
    template="사용자가 다음 명령을 내렸습니다: {command}. 어떤 클래스만 탐지해야 하나요?"
)

llm_chain = LLMChain(
    llm=llm,
    prompt=prompt_template
)

def process_command(command):
    try:
        response = llm_chain.run(command)
        response_text = response.strip()
        print(f"Response from OpenAI: {response_text}")
        
        # '횡단보도'와 '빨간불' 감지 요청에 대한 처리
        if "횡단보도" in response_text and "빨간불" in response_text:
            return ["횡단보도", "빨간불"]
        else:
            return []
    except Exception as e:
        print(f"Error processing command: {str(e)}")
        return []

def filter_detections_by_class(detections, classes):
    filtered_results = []
    for det in detections:
        boxes = det.boxes
        for i in range(len(boxes.xyxy)):
            cls_id = int(boxes.cls[i])
            label = model.model.names[cls_id]
            if label in classes:
                filtered_results.append({
                    'box': boxes.xyxy[i].tolist(),
                    'label': label,
                    'confidence': boxes.conf[i].item()
                })
    return filtered_results

def detect_objects_in_video(video_source, desired_classes, volume):
    cap = cv2.VideoCapture(video_source)
    engine.setProperty('volume', volume)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # YOLO를 사용하여 프레임에서 객체 탐지
        results = model.predict(frame, conf=0.5, vid_stride=7)
        filtered_results = filter_detections_by_class(results, desired_classes)

        # 프레임을 PIL 이미지로 변환
        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(frame_pil)

        # 현재 프레임에서의 상태 변수
        is_crosswalk_detected = False
        is_red_light_detected = False
        is_green_light_detected = False

        # 필터링된 결과 순회
        for det in filtered_results:
            x1, y1, x2, y2 = det['box']
            label = det['label']
            conf = det['confidence']
            color = colors.get(label, (128, 128, 128))  # 기본 색상은 회색

            # 바운딩 박스 및 텍스트 그리기
            draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
            text = f"{label}: {conf:.2f}"
            bbox = draw.textbbox((x1, y1), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            draw.text((x1, y1 - text_height), text, fill=color, font=font)

            # 상태 업데이트 및 음성 큐 추가
            if label == '횡단보도' and conf >= 0.6:
                is_crosswalk_detected = True
                q.put(f"횡단보도가 감지되었습니다")
            if label == '빨간불' and conf >= 0.6:
                is_red_light_detected = True

        # 음성 출력 조건 확인 및 큐에 추가
        if is_crosswalk_detected:
            if is_red_light_detected and current_light_state != 'red':
                while not q.empty():
                    q.get()
                current_light_state = 'red'
                q.put("빨간불이니 기다려 주세요")
            elif is_green_light_detected and current_light_state != 'green':
                while not q.empty():
                    q.get()
                current_light_state = 'green'
                q.put("초록불로 바뀌었으니 길을 건너세요")

        # PIL 이미지를 다시 OpenCV 이미지로 변환
        frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)

        # OpenCV 창에 프레임 출력
        cv2.imshow('YOLO Object Detection', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# Streamlit UI 설정
st.title("YOLO Object Detection App")

video_source = st.radio("비디오 소스 선택", ('캠코더', '비디오 파일'))

if video_source == '비디오 파일':
    video_file = st.file_uploader("비디오 파일 업로드", type=["mp4", "mov", "avi"])
    if video_file:
        with open("temp_video.mp4", "wb") as f:
            f.write(video_file.getbuffer())
        st.success("비디오 파일 업로드 완료")

classes = st.text_input("탐지할 클래스 입력 (예: 횡단보도, 빨간불)", "")

volume = st.slider("음량 조절", 0, 100, 50)  # 음량 조절 슬라이더

if st.button("탐지 시작"):
    desired_classes = [cls.strip() for cls in classes.split(",")]
    if video_source == '캠코더':
        threading.Thread(target=detect_objects_in_video, args=(0, desired_classes, volume / 100.0)).start()
    else:
        threading.Thread(target=detect_objects_in_video, args=("temp_video.mp4", desired_classes, volume / 100.0)).start()

# 음성 출력 스레드 종료
q.put(None)
speak_thread.join()

print("Processing complete.")
